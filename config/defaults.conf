// Default config file.
// Set values here, then override them in custom configs by including this at
// the top, e.g.
// my_expt.conf:
//   include "defaults.conf"
//
//   exp_name = "my_expt"
//   run_name = "run1"
//   n_layers_env = 2
//   word_embs = "fastText"
//

// Logistics
cuda = 0  // gpu id, or -1 to disable
random_seed = 19  // (global) random seed

// Paths and logging
exp_name = "common-indexed-tasks"  // experiment name
run_name = "tuning-0"  // run name
data_dir = ${JIANT_DATA_DIR}  // required
exp_dir = ${JIANT_PROJECT_PREFIX}"/"${exp_name}"/"  // required
run_dir = ${JIANT_PROJECT_PREFIX}"/"${exp_name}"/"${run_name}  // required
log_file = "log.log"  // log file, goes in run directory
// log name for remote logging; make as unique as possible
remote_log_name = ${exp_name}"__"${run_name}

// Execution control
do_train = 1    // run training steps
do_eval = 1     // run eval steps
load_model = 1  // if true, restore main training from checkpoint if available
load_eval_checkpoint = "none"  // path to eval checkpoint, or "none"
reload_tasks = 0     // if true, force reloading tasks
reload_indexing = 0  // if true, force reindexing tasks
reload_vocab = 0     // if true, force vocabulary rebuild

// Tasks and task-specific modules
train_tasks = "mnli"  // required, comma-separated list of training tasks,
                      // or 'all' or 'none'
eval_tasks = "qqp,cola,wnli,qnli,sst,sts-b,rte,mrpc"   // required, additional eval tasks
train_for_eval = 1  // if true and if needed, train new classifiers for eval tasks
classifier = "mlp"    // classifier type
classifier_hid_dim = 512  // classifier width
classifier_dropout = 0.2  // classifier dropout rate
d_hid_dec = 300           // decoder hidden size
n_layers_dec = 1          // decoder number of layers

// Preprocessing options
max_seq_len = 40         // maximum (input) sequence length
max_word_v_size = 30000  // maximum word vocab size
max_char_v_size = 250    // maximum char vocab size

// Embedding options
word_embs = "none"  // type of embeddings: 'none', 'scratch', 'glove, 'fastText'
word_embs_file = ${WORD_EMBS_FILE}  // path to embeddings file
                                    // shouldn't this be $FASTTEXT_EMBS_FILE ?
fastText = 0                                  // if true, use fastText model
fastText_model_file = ${FASTTEXT_MODEL_FILE}  // path to fastText model
d_word = 300  // dimension of word embeddings
d_char = 100  // dimension of char embeddings
n_char_filters = 100  // number of filters in char CNN
char_filter_sizes = "2,3,4,5"  // size of char filters
elmo = 1             // if true, use ELMo
elmo_chars_only = 1  // if true, use only char CNN layer of ELMo
cove = 0             // if true, use CoVe
char_embs = 0        // if true, use char embeddings
dropout_embs = 0.2   // dropout rate for embeddings
preproc_file = "preproc.pkl"  // file to save preprocessing data
                              // TODO(alex): document what goes in here?

// Model options
sent_enc = "rnn"  // type of sentence encoder: 'bow', 'rnn', or 'transformer'
                  // TUNE ME: bow or rnn or transformer
sent_combine_method = "max"  // pooling method for encoder states:
                             // 'max', 'mean', or 'final'
bidirectional = 1    // if true, use bidirectional RNN
pair_attn = 1  // 1 to use attn in pair classification/regression tasks
shared_pair_attn = 0  // if true, share pair attn for pairwise tasks
d_hid = 512          // hidden dimension size (usually num_heads * d_proj for transformer)
                     // TUNE ME:
                     //   BiLSTM: 1500
                     //   Transformer: hid768/heads12 or hid512/heads8 or hid384/heads6
d_hid_attn = 512     // post-attention LSTM state size
d_proj = 512         // task-specific linear project before pooling
                     // TODO: Should be set per task.
n_layers_enc = 2     // number of encoder layers
                     // TUNE ME:
                     //   BiLSTM: 2
                     //   Transformer: 12
skip_embs = 1        // if true, adds skip connection to concatenate encoder
                     // input to encoder output
                     // TUNE ME
n_layers_highway = 0  // number of highway layers
                      // TODO(alex): Where are these layers in the model?
n_heads = 8  // number of transformer heads
             // TUNE ME: See above.
d_tproj = 64  // transformer projection dimension
d_ff = 2048  // transformer feed-forward dimension
dropout = 0.2  // dropout rate

// Training options
no_tqdm = 1  // if true, disable tqdm progress bar
trainer_type = "sampling"  // type of trainer: 'sampling'
shared_optimizer = 1  // if true, use same optimizer for all tasks
batch_size = 32   // training batch size
                  // TODO: Adjust as needed for memory.
optimizer = "adam" // optimizer. All valid AllenNLP options are available,
                   // including 'sgd'. 'adam' uses the newer AMSGrad variant.
lr = 0.0001          // initial learning rate
min_lr = 0.000001  // (1e-6) minimum learning rate
max_grad_norm = 5.0  // maximum gradient norm
weight_decay = 0.0000001   // weight decay value
task_patience = 2    // patience in decaying per-task learning rate
scheduler_threshold = 0.0001 // threshold used in deciding when to lower learning rate
lr_decay_factor = 0.5  // learning rate decay factor, when validation score
                       // doesn't improve
warmup = 4000  // number of warmup steps for transformer LR schedule

val_interval = 5000  // number of passes between validation checks
max_vals = 25     // maximum number of validation checks
bpp_base = 1       // number of batches to train per sampled task
patience = 10  // patience in early stopping
keep_all_checkpoints = 0 // If set, keep checkpoints from every validation.
                         // Otherwise, keep only best and (if different) most recent.

// Multi-task training options
weighting_method = "proportional"  // weighting method for sampling:
                                   // 'uniform' or 'proportional'
scaling_method = "max"   // method for scaling loss:
                         // 'min', 'max', 'unit', or 'none'

// Evaluation options
eval_val_interval = 500  // validation interval for eval task
                         // TODO: Tune this per task. 500 is probably high for RTE and low for QQP
eval_max_vals = 50      // maximum number of validation checks for eval task
write_preds = 0  // if true, write test predictions; will likely break for non-GLUE tasks


// Tasks specific stuff: either task-specific model parameters or eval-time trainer params
sts-b_d_proj = 256
sts-b_classifier_hid_dim = 128
sts-b_classifier_dropout = 0.4
sts-b_pair_attn = 0
sts-b_lr = .0001
sts-b_max_vals = 50             // only affects eval-training
sts-b_val_interval = 100        // only affects eval-training

mrpc_d_proj = 256
mrpc_classifier_hid_dim = 128
mrpc_classifier_dropout = 0.4
mrpc_pair_attn = 0
mrpc_lr = .0001
mrpc_max_vals = 50
mrpc_val_interval = 100

rte_d_proj = 256
rte_classifier_hid_dim = 128
rte_classifier_dropout = 0.4
rte_pair_attn = 0
rte_lr = .0001
rte_max_vals = 50
rte_val_interval = 100

wnli_d_proj = 256
wnli_classifier_hid_dim = 128
wnli_classifier_dropout = 0.4
wnli_pair_attn = 0
wnli_lr = .0001
wnli_max_vals = 50
wnli_val_interval = 100

qqp_classifier_hid_dim = 512
qqp_classifier_dropout = 0.2
qqp_pair_attn = 1
qqp_lr = .0001
qqp_max_vals = 15
qqp_val_interval = 10000

qnli_classifier_hid_dim = 512
qnli_classifier_dropout = 0.2
qnli_pair_attn = 1
qnli_lr = .0001
qnli_max_vals = 25
qnli_val_interval = 5000

mnli_classifier_hid_dim = 512
mnli_classifier_dropout = 0.2
mnli_pair_attn = 1
mnli_lr = .0001
mnli_max_vals = 15
mnli_val_interval = 10000

cola_d_proj = 256
cola_classifier_hid_dim = 512
cola_classifier_dropout = 0.2
cola_lr = .0001
cola_max_vals = 25
cola_val_interval = 100

sst_d_proj = 256
sst_classifier_hid_dim = 512
sst_classifier_dropout = 0.2
sst_lr = .0001
sst_max_vals = 25
sst_val_interval = 500
